{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img src=\"img\\imdb.png\" >\n",
    "<br>\n",
    "\n",
    "Utilize o dataset train.csv para os exercicios.\n",
    "\n",
    "# Exercício: \n",
    "\n",
    "Estamos em 2015 e você trabalha em uma empresa que compra filmes para passar no cinema. Bons filmes custam caro, assim como filmes ruins barato, mas Bons filmes atraem mais público. Seu chefe sabe que você está aprendendo Machine Learning e te propõe a seguinte tarefa:  - Precisamos achar uma boa oportunidade que nos faça aumentar o lucro da empresa!\n",
    "\n",
    "Você conseguirá aumentar o lucro da empresa quando comprar um filme que é subvalorizado mas que as features dele indicam que será um bom filme.\n",
    "\n",
    "Se pudessemos descobrir quais filmes que as pessoas acham que seriam ruins mas na verdade são bons, poderiamos investir apenas nestes e ganhar a diferença já que foi mal precificado.\n",
    "\n",
    "\n",
    "Vamos partir dos seguintes pressupostos:\n",
    "- Um filme bom ou ruim do ponto de vista de quem está precificando será o tamanho do seu orçamento (budget).\n",
    "\n",
    "- Um filme Bom/Ruim do ponto de vista do público serão filmes com altas notas (imdb_score).\n",
    "\n",
    "Então vamos procurar os filmes que tenham uma boa relação entre budget e score.\n",
    "(é como se quisessemos comprar scores pois isso que o público quer ver mas os donos dos filmes vendem baseado no budget do filme).\n",
    "\n",
    "O primeiro passo é entendermos bem o problema e os dados disponíveis.\n",
    "\n",
    "Por exemplo, se vamos comprar um filme, algumas features não estarão disponiveis antes da compra (elas são criadas apenas após a compra) para predizer o rating como as vendas de ingresso, faturamento, etc.\n",
    "\n",
    "Depois separamos nos nossos dados quais serão nossas variáveis explicativas (Os X) e qual é nossa variável target (nosso Y).\n",
    "\n",
    "Caso algumas das nossas variáveis explicativas sejam categórias, precisamos transforma-las em numéricas para que o modelo consiga entende-la.\n",
    "\n",
    "Podemos utilizar uma função do pandas dessa forma:\n",
    "\n",
    "``` df = pd.get_dummies(df) ```\n",
    "\n",
    "O último passo antes de rodar um modelo é dividir nossos dados em duas partes, uma para treinar o modelo e outra para testa-lo, podemos fazer uma amostra aleatória ou usar a função do sklearn que nos ajuda com isso:\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    " X_train, X_test, y_train, y_test = train_test_split(df[X], df[y], test_size=0.33, random_state=42)\n",
    "```\n",
    "Para isso Treine um Modelo de Machine Learning supervisionado que aprenda a predizer as notas dos filmes.\n",
    "\n",
    "Utilize os 4 passos visto na primeira aula de machine learning:\n",
    "```from sklearn.linear_model import LinearRegression```\n",
    "\n",
    "Após treinar o modelo vamos ver como estão as [métricas](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) de assertividade.\n",
    "\n",
    "Analisar o R2, RMSE, MSE. Qual é o melhor nesse caso?\n",
    "\n",
    "Exemplo do MSE\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y_true, y_pred)\n",
    "```\n",
    "\n",
    "Como poderiamos melhorar esse modelo? Será que existe algum método para colocar todas as variáveis ao quadrado, cubo, etc para que possamos capturar efeitos não lineares? Óbviamente existe e já foi implementado no sklearn, podemos usar a função:\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "\n",
    "df = poly.fit_transform(df)\n",
    "```\n",
    "\n",
    "Aparentemente sempre que aumentamos o grau do polinomio nosso modelo fica melhor, porque não podemos botar um número alto como 10?\n",
    "\n",
    "Rode o algoritmo com um grau = 10 e compare as métricas de treino contra as métricas de teste. O que está acontecendo?\n",
    "\n",
    "Parece que estamos chegando perto de um modelo bom para nosso objetivo de identificar oportunidade nos filmes. Será que poderiamos utilizar outro método de regressão mais robusto? Vamos tentar utilizar um dos melhores métodos que utiliza arvores aleatórias (usado geralmente em classificação e as estudaremos mais profundamente no módulo 4) como modelo:\n",
    "\n",
    "Treine agora o mesmo modelo utilizando um modelo não-linear (não exige que as correlações sejam lineares) e compare as métricas.\n",
    "\n",
    "```from sklearn.ensemble import RandomForestRegressor```\n",
    "\n",
    "\n",
    "\n",
    "Agora estamos em 2016. Use seu modelo treinado com todos os filmes até 2015 para responder a pergunta do seu chefe com os filmes que apareceram em 2016: Quais são os filmes que são realmente oportunidades de ganhar dinheiro (altos ratings, baixo custo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 1 - Entender o problema\n",
    "\n",
    "O problema está descrito no tutorial acima. O que não está escrito é qual abordagem deveriamos ter para resolve-lo.\n",
    "\n",
    "Vamos pensar em uma possibilidade de solução aqui e escreveremos todo o restante do código pensando em como alcançar esse resultado.\n",
    "\n",
    "Não existe um só caminho para resolver esse problema, mas a solução que eu pensei em aplicar é a seguinte:\n",
    "\n",
    "    1) de um lado os donos dos filmes precificam seus produtos com base em quanto gastaram.\n",
    "\n",
    "    2) Seguindo o pressuposto do problema, o sucesso de bilheteria do filme depende apenas das notas que as pessoas vão dar a ele.\n",
    "\n",
    "Com isso chegamos a conclusão que o melhor filme para apostarmos são aqueles que a relação entre o preço (1-custo) e o (2-receita) é a mais vantajoso para nós.\n",
    "\n",
    "Uma forma de pensar é que temos que \"comprar\" scores então procuraremos os scores mais baratos que será dado por (1)/(2) -> custo do filme / beneficio do filme.\n",
    "\n",
    "Então precisamos treinar um modelo de machine learning que quando chegar um filme novo, prediga com o máximo de acurácia a nota média que as pessoas darão para ele ( o quociente na nossa equação) já que o númerador (o custo do filme) já saberemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improtando as bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados e armazenando em uma variável\n",
    "df_raw = pd.read_csv(\"../../99 Datasets/imdb_train.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma cópia para o DataFrame\n",
    "\n",
    "Agora vamos fazer uma cópia do nosso dataframe; note que usamos a função deep=True pois só assim garantimos que o dado está duplicado e não é apenas um \"atalho\" para o dataset original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos fazer uma cópia do nosso dataframe.\n",
    "df = df_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando o tamanho dos dados\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 2 - Escolher as variáveis.\n",
    "\n",
    "\n",
    "Definido claramente o que vamos fazer, vamos olhar as variáveis disponiveis e ver o que faz ou não sentido incluir.\n",
    "\n",
    "Diferentemente de modelagem estatistica, em machine learning não estamos preocupados com as variáveis serem correlacionadas, então não há um bom motivo (por hora) para retirarmos variáveis do modelo, portanto, meu primeiro modelo de 'benchmark' terá o máximo de variáveis/features possível.\n",
    "\n",
    "Algumas são facilmente excluidas, pois mesmo que sejam boas preditoras como o tamanho da bilheteria, ela só estará disponível após o filme ser lançado. Parte bastante importante da definição de um modelo de machine learning é pensar nesses pontos, de quais variáveis estarão disponiveis em um modelo de produção e isso, muitas vezes, pode ser bastante complicado por conta do carater temporal das variáveis tornando-se um \"leakage\" ou seja, um vazamento de dados.\n",
    "\n",
    "\n",
    "Variáveis como gross sabemos que não estará disponivel antes do lançamento então não vamos usa-la para treinar nosso modelo.\n",
    "\n",
    "Outra variável que não faz sentido para um modelo de machine é aquela que identifica o filme como um id, index ou o nome do filme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listando as colunas do DataFrame\n",
    "list(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escolhendo colunas para dropar\n",
    "columns_to_drop = ['Unnamed: 0', 'gross', 'movie_title', 'plot_keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo um novo DataFrame com base no original\n",
    "df = df_raw.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listando as colunas do novo DataFrame\n",
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 3 - Tratando as variáveis.\n",
    "\n",
    "**Essa é a etapa mais longa, mais importante e mais dificil de um problema de machine learning.** (falaremos mais sobre essas técnicas nas próximas aulas)\n",
    "\n",
    "Essa etapa podemos usar todos os tratamentos de variáveis que aprendemos na modelagem estatistica quando estavamos rodando regressões simples. Não vou repetir os tratamentos mas todos podem ser utilizados.\n",
    "\n",
    "\n",
    "## Campos nulos\n",
    "\n",
    "Quando rodamos um dff.describe() percebemos muitos campos nulos e há diversas formas de trata-los.\n",
    "\n",
    "Alternativas para tratar nulos.\n",
    "\n",
    "1) Deleter todos os nulos.\n",
    "\n",
    "```\n",
    "df = df.dropna()\n",
    "```\n",
    "\n",
    "2) Substituir os nans pelas médias (nas numéricas) ou modas (nas categóricas).\n",
    "\n",
    "df['numericas'] = df['numericas'].fillna(df.mean())\n",
    "\n",
    "\n",
    "3) rodar um \"pré-ML\" que prevê os valores faltantes.\n",
    "\n",
    "\n",
    "\n",
    "4) Usar um algoritmo robusto para campos nulos.\n",
    "\n",
    "ex:\n",
    "\n",
    "```\n",
    "import xgboost as xgb\n",
    "```\n",
    "\n",
    "Vamos usar uma abordagem mista. Nas variáveis numéricas vamos substituir pela média e nas variáveis categóricas vamos excluir as linhas com campos nulos para aprender as duas metodologias. \n",
    "\n",
    "(conforme vimos em estatistica, excluir linhas que não são aleatórias enviesam nosso modelo e não deve ser feito a menos que você tenha certeza que aqueles nulos são por alguma falha aleatória o que é bastante raro. Não sendo aleatório devemos tratar com uma das outras metodologias, principalmente a número 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando os tipos das variáveis\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo um dataset apenas com colunas numéricas\n",
    "df_numeric = df.select_dtypes(include=[np.number]) \n",
    "numericas = list(df_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa função selecionamos todas as colunas númericas  \n",
    "- substituimos seus valores pelas médias das colunas\n",
    "- A função fillna retorna todos os campos nulos \n",
    "- função df.mean retorna as médias de cada coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecionando as colunas numéricas e preenchendo com a média\n",
    "df[numericas] = df[numericas].fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já nas variáveis categóricas (que contém texto ao invés de números), vamos substituir os campos nulos por uma nova string que representa que aquele campo é faltante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitui os dados faltantes por 'na'\n",
    "df = df.fillna('na') \n",
    "# se sobrar alguma linha (nao deveria), dropamos essas linhas.\n",
    "df = df.dropna() \n",
    "# verificando o tamanho do DataFrame\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar variáveis dummies para features categóricas.\n",
    "\n",
    "Basicamente os algoritmos de machine learning não conseguem interpretar textos como o genêro de um filme ou os atores que participaram então precisamos transformar essas colunas categóricas em colunas numéricas para que o algoritmo possa capturar seus efeitos e predizer nosso target.\n",
    "\n",
    "Substituir as categorias por números dessa forma: COluna genêro de filme com valores [Terror=1, Comédia=2, Ação=3] embora faça o algoritmo rodar, não funcionará bem como variável explicativa pois elas não representam uma sequencia numérica. A forma mais simples de fazermos isso virar uma variável explicativa é pegar cada uma das categorias possíveis e transformar em uma coluna forma única que recebe apenas dois valores, 0 e 1. 0 se o valor não estiver presente e 1 caso esteja presente. Dessa forma nossa  representação ficará da seguinte forma. \n",
    "\n",
    "- Coluna 1 = genero_terror\n",
    "- Coluna 2 = genero_comédia\n",
    "- Coluna 3 = genero_ação\n",
    "\n",
    "Assumindo 1 ou 0 dependendo do filme.\n",
    "\n",
    "Uma observação sobre essa técnica é que não é necessário colocar todas as possibilidades, uma delas se torna redundante e ao modelar esse caso podemos excluir uma delas.\n",
    "\n",
    "Essa técninca chamamos de one_hot (sklearn) e no pandas temos a função get_dummies que faz o mesmo processo.\n",
    "\n",
    "Outra etapa de tratamento das variáveis consiste em transformar as variáveis em funções lineares (se estivermos rodando modelos lineares) através da aplicação de log nas variáveis \"explosivas\" tornando-a linear. Não faremos essa etapa pois nosso objetivo é rodar um algoritmo não linear em seguida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplicação do get_dummies do pandas para dummização de variáveis categóricas\n",
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificação do tamamno do DataSet\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentamos das 26 colunas originais para mais de 15mil colunas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 4 - Treinando o modelo de Machine Learning\n",
    "\n",
    "Essa é a etapa mais simples mas que exige mais experiência.\n",
    "\n",
    "1) qual algoritmos escolher?\n",
    "\n",
    "Sabemos que nosso problema se trata de prever uma variável continua (uma nota) e como temos diversas notas para treina-lo podemos usar uma abordagem supervisionado.\n",
    "\n",
    "(obs: poderiamos tentar transformar cada intervalo de notas em uma categoria, ex: [0,1] -> 1, [1,2] -> 2 .... [9,10] -> 10 e prever usando um algoritmo de classificação, mas como as notas fazem sentido como uma variável numérica, ou seja, a nota seguinte é a nota anterior + 1, um algoritmo de classificação dificilmente superaria um de regressão já que as probabilidades de cada classe prevista em uma classificação é independente, é como se uma nota não tivesse nenhuma relação com a outra e isso não é verdade no nosso caso).\n",
    "\n",
    "Então:\n",
    "\n",
    "    Supervisionado -> Regressão\n",
    "    \n",
    "    \n",
    "Mas há dezenas de algoritmos de regressão (ver todos os disponiveis no sklearn nesse link: http://scikit-learn.org/stable/supervised_learning.html )\n",
    "\n",
    "Alguns deles:\n",
    "- MQO (o mais simples que estudamos em estatistica e abrimos o código na aula2 de machine learning).\n",
    "- Ridge -> Método de Shrinkage. Conrola sobreajuste com L1\n",
    "\n",
    "    $L = ∑( Ŷi– Yi)^2 + λ∑ β^2$\n",
    "\n",
    "\n",
    "- lasso -> Também método de Shrinkage. controle sobreajuste com L2\n",
    "\n",
    "   $ L = ∑( Ŷi– Yi)^2 + λ∑ |β|$\n",
    "\n",
    "\n",
    "- elasticnet -> combinação de Lasso + Ridge\n",
    "- Regressão Bayesiana -> Retorna distribuições de probabilidade ao invés de simples valores.\n",
    "\n",
    "    Ex: https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7?gi=392eaf9ea3b \n",
    "    \n",
    "- Arvores aleatórias -> insere não lineariedades\n",
    "- Redes neurais -> também insere não lineariedades.\n",
    "- Ensambles -> Combinação de diferentes modelos.\n",
    "- Etc\n",
    "\n",
    "\n",
    "Qual usar ?\n",
    "\n",
    "No nosso exemplo vamos usar o mais simples MQO para usar de comparativo e um extremamente poderoso que são as arvores aleatórias (que ainda não vimos seu funcionamento).\n",
    "\n",
    "Na prática, quando colocamos um modelo em produção, podemos rodar diferentes algoritmos (que tem suas próprias vantagens e desvantagens) e pegar uma combinação desses diferentes algoritmos como o resultado final. A essa combinação de modelos damos o nome de **ensamble**.\n",
    "\n",
    "Para rodar qualquer modelo de machine learning desses, vamos passar pelos mesmos passos:\n",
    "\n",
    "    1) Importar o modelo desejado.\n",
    "    2) Instanciar em uma variável com os parâmetros desejados (ainda não vimos os parâmetros).\n",
    "    3) Separar nossos dados em variáveis explicativas (X) e explicadas/target (Y)\n",
    "    4) Separar nossos dados em Treino e Teste\n",
    "    5) Treinar o Modelo com o .fit()\n",
    "    6) Analisar as métricas, se não estiver boas, voltamos aos passos anteriores de trabalhar com as variáveis.\n",
    "    7) Estando tudo ok, rodamos previsões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando os modelos\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciar os modelos em variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando os modelos\n",
    "modelo_MQO = LinearRegression()\n",
    "modelo_RF = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar os dados em X e Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando os dados em dados em X e Y\n",
    "X = df.drop(['imdb_score'], axis = 1)\n",
    "Y = df['imdb_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar os dados em Treino e Teste \n",
    "\n",
    "Esse ponto é crucial em machine learning e falaremos mais vezes sobre diferentes técnicas em \"splitar\" os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca de split de dados do Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "# separando os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimindo o tamanho dos DataSet\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar o Modelo \n",
    "\n",
    "Para que o algoritmo descrubra os melhores betas (que produzem os menores erros) - ou os melhores otimizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treinando o modelo linear\n",
    "modelo_MQO.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculando e analisando as métricas\n",
    "\n",
    "Após treinar o modelo podemos analisar suas métricas e aqui é uma parte sensivel, vamos entender algumas métricas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bibliotecas\n",
    "from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error\n",
    "# previsao dos dados de treino para calcular as métricas\n",
    "yhat_train = modelo_MQO.predict(X_train) \n",
    "# previsao dos dados de teste para calcular métricas\n",
    "yhat_test = modelo_MQO.predict(X_test) \n",
    "\n",
    "# imprimindo as métricas de treino\n",
    "print('-----Dados de Treino-----')\n",
    "print('MSE - treino', mean_squared_error(y_train, yhat_train))\n",
    "print('MAE - treino', median_absolute_error(y_train, yhat_train))\n",
    "print('R2 - treino', r2_score(y_train, yhat_train))\n",
    "# imprimindo as métricas de teste\n",
    "print('\\n-----Dados de Teste-----')\n",
    "print('MSE - test', mean_squared_error(y_test, yhat_test))\n",
    "print('MAE - test', median_absolute_error(y_test, yhat_test))\n",
    "print('R2 - test', r2_score(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos entender primeiro as métricas e em seguida entender a diferença entre os números de treino e de teste.\n",
    "\n",
    "**O que é o MSE e o MAE**\n",
    "MSE: Erros médios quadraticos.\n",
    "MAE: Erros médios absolutos.\n",
    "\n",
    "Qual a diferença entre eles?\n",
    "\n",
    "Pensemos no caso de um dataset com muitos outlinears, qual das duas métricas será mais prejudica (ou seja, qual métrica é mais sensivel a dados muito fora do padrão?). Nosso métrica ao quadrado é mais sensivel pois pegamos a diferença entre o valor predito e o realizado e elevamos ao quadrado tornando a diferença ainda maior. Já o MAE não é tão sensivel aos valores extremos pois consideramos apenas a distância absoluta que da o mesmo peso a distâncias grandes e pequenas.\n",
    "Ex: Valor predito = 8, Valor Real = 5.\n",
    "MAE= 8-5 = 3\n",
    "MSE= (8-5)^2 = 9 -> Penaliza erros maiores.\n",
    "\n",
    "\n",
    "Já o R2 é a mesma intepretação que demos em modelagem estatistica (% da variação de Y explicada por todas as variáveis explicativas X). Só que agora nosso objetivo é aumentar ao máximo nosso R2.\n",
    "\n",
    "\n",
    "\n",
    "** Métricas de Treino e Métricas de Teste **\n",
    "\n",
    "O grande objetivo de um modelo de aprendizado de maquina (ML) é termos um modelo que é preditivo para novos dados, ou seja, precisamos de um modelo que seja bastante genérico, que depois de treinado pode pegar dados que não estavam na amostra e consiga prever seu target com uma boa precisão.\n",
    "\n",
    "Então ter boas métricas nos dados de treino não significam necessariamente que o modelo é um bom preditor. Para isso ele precisa ter boas métricas nos dados de teste! (aquela parte dos dados que separamos e não participaram do treino). Se nosso algoritmo é capaz de prever de maneira satisfatória nossos dados de teste (que são dados novos para ele já que não participaram do treino) esse é um bom modelo.\n",
    "\n",
    "Quando as métricas dos dados de treino estão muito superiores aos dados de teste (como no nosso exemplo) significa que estamos fazendo um superajuste/sobreajuste ou como gostamos de chamar o modelo está com overfiting. Significa basicamente que nosso modelo se tornou especialista em prever nossos dados de treino mas não necessariamente é um bom modelo para prever dados fora do treinamento. Há muitas formas de contornar isso que veremos nas próximas aulas, mas a mais importante que faremos aqui é tentar equilibrar a complexidade do modelo, basicamente colocamos mais variáveis do que seria necessario.\n",
    "\n",
    "É aqui que voltamos ao passo de escolher e tratar variaveis e rodamos tudo novamente.\n",
    "Vamos fazer todos os processos na próxima linha e colocar um modelo um pouco mais simples:\n",
    "\n",
    "\n",
    "<img src=\"img\\overfit.png\" style=\"height:250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melhorando o ajuste do modelo\n",
    "\n",
    "Vamos remover algumas colunas que podem estar causando um superajuste/overfiting no nosso modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listando as colunas do DataFrame\n",
    "list(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estratégia de eliminação de colunas\n",
    "\n",
    "Para o DataSet do IMDB, vamos manter as colunas com likes no título pois representam uma característica importante dos filmens, e remover as colunas com nomes, links, unnamed e keywords pois são muito específicas e não diferenciam os filmas entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primeiro vamos identificar as colunas\n",
    "# função para manutenção e eliminação de colunas\n",
    "for column in list(df_raw):\n",
    "    if 'likes' in column:\n",
    "        pass\n",
    "    elif 'name' in column or 'link' in column or 'Unnamed: 0' in column or 'plot_keywords' in column:\n",
    "        print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos eliminar as colunas\n",
    "\n",
    "# definindo uma lista para as colunas\n",
    "overfiting_columns = []\n",
    "# aplicação da função para escolher as colunas a serem eliminadas\n",
    "for column in list(df):\n",
    "    if 'likes' in column:\n",
    "        pass\n",
    "    elif 'name' in column or 'link' in column or 'Unnamed: 0' in column:\n",
    "        overfiting_columns.append(column)\n",
    "\n",
    "# imprimindo as colunas a serem eliminadas\n",
    "print(f'#columns_to_drop: {len(overfiting_columns)}')\n",
    "# dropando as colunas selecionadas no DataFrame original\n",
    "df = df.drop(overfiting_columns, axis=1)\n",
    "# criando as variáveis Dummies\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# imprimindo o novo tamanho do DataFrame\n",
    "print(f'new_shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando novamente uma Regressão Linear\n",
    "\n",
    "Vamos verificar como os dados respondem a uma nova regressão linear considerando o DataFrame sem as colunas removidas para eliminação do Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodando uma regressão linear através de uma função\n",
    "def run_mqo(df, model):\n",
    "    # separando os dados em X e Y\n",
    "    X = df.drop(['imdb_score'], axis = 1)\n",
    "    Y = df['imdb_score']\n",
    "    # split dos dados em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25) \n",
    "    # instanciando o modelo\n",
    "    model.fit(X_train,y_train)\n",
    "    # fazendo previsões com os dados de treino \n",
    "    yhat_train = model.predict(X_train) \n",
    "    # fazendo previsões com os dados de teste\n",
    "    yhat_test = model.predict(X_test[list(X_train)])\n",
    "    \n",
    "    # imprimindo as métricas para os dados de treino\n",
    "    print('-----Dados de Treino-----')\n",
    "    print('MSE - treino', mean_squared_error(y_train, yhat_train))\n",
    "    print('MAE - treino', median_absolute_error(y_train, yhat_train))\n",
    "    print('R2 - treino', r2_score(y_train, yhat_train))\n",
    "    \n",
    "    # imprimindo as métricas para os dados de teste\n",
    "    print('\\n-----Dados de Teste-----')\n",
    "    print('MSE - test', mean_squared_error(y_test, yhat_test))\n",
    "    print('MAE - test', median_absolute_error(y_test, yhat_test))\n",
    "    print('R2 - test', r2_score(y_test, yhat_test))\n",
    "\n",
    "# rodando a função criada para rodar uma regressão linear\n",
    "run_mqo(df, LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rodando novamente uma Random Forrest\n",
    "\n",
    "Vamos passar agora agora um segundo modelo que consegue capturar, entre outras coisas, não lineariedades, uma vez que o modelo linear não está respondendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizando a mesma função para apliação de uma Random Forrest\n",
    "run_mqo(df, RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece bem melhor agora, tanto o R2 aumentou como ficou mais parecido entre treino e teste sugerindo que não estou dando overfiting no modelo.\n",
    "\n",
    "Podemos ainda criar modelos um pouco mais complexos inserindo formas quadraticas, cubicas, etc para tentar capturar algum efeito não linear nos dados.\n",
    "\n",
    "Idealmente plotariamos as variáveis e escolheriamos as que fazem mais sentido colocar variaveis quadraticas, etc. Mas na prática rodamos um algoritmo para passar um polinomio em todas as nossas variáveis como no código abaixo:\n",
    "\n",
    "Obs: Colocamos todos os passos da regressão em uma função para não termos que ficar repetindo mais códigos.\n",
    "\n",
    "## Definindo o melhor modelo para as utilização nas predições finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando uma função para imprimir as métricas do modelo\n",
    "def print_metrics(y_train, yhat_train,y_test, yhat_test):\n",
    "    # imprimindo as métricas de treino\n",
    "    print('\\n-----Dados de Treino-----')\n",
    "    print('MSE - treino', mean_squared_error(y_train, yhat_train))\n",
    "    print('MAE - treino', median_absolute_error(y_train, yhat_train))\n",
    "    print('R2 - treino', r2_score(y_train, yhat_train))\n",
    "    # imprimindo as métricas de teste\n",
    "    print('\\n-----Dados de Teste-----')\n",
    "    print('MSE - test', mean_squared_error(y_test, yhat_test))\n",
    "    print('MAE - test', median_absolute_error(y_test, yhat_test))\n",
    "    print('R2 - test', r2_score(y_test, yhat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bibliotecas\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# definindo uma variável com as colunas numéricas\n",
    "numeric_columns = ['num_critic_for_reviews',\n",
    "                     'duration',\n",
    "                     'director_facebook_likes',\n",
    "                     'actor_3_facebook_likes',\n",
    "                     'actor_1_facebook_likes',\n",
    "                     'num_voted_users',\n",
    "                     'cast_total_facebook_likes',\n",
    "                     'facenumber_in_poster',\n",
    "                     'num_user_for_reviews',\n",
    "                     'budget',\n",
    "                     'title_year',\n",
    "                     'actor_2_facebook_likes',\n",
    "                     'aspect_ratio',\n",
    "                     'movie_facebook_likes']\n",
    "\n",
    "# criando uma lista vazia para variáveis categóricas\n",
    "dummies = []\n",
    "\n",
    "# colocando as vairáveis categóricas em uma lista\n",
    "for column in list(df):\n",
    "    if 'imdb_score' in column:\n",
    "        pass\n",
    "    elif column not in numeric_columns:\n",
    "        dummies.append(column)\n",
    "\n",
    "# criando uma função para modelagem com escolha do grau de transformação polinomial \n",
    "def run_model_with_poly(dataframe, poly_n, modelo):\n",
    "    # definição do grau da função polinomial de acordo com o parâmetro inicial   \n",
    "    poly = PolynomialFeatures(degree=poly_n)\n",
    "    # definição dos dados de entrada X e imprimindo o tamanho da variável\n",
    "    newX = dataframe[dummies+numeric_columns]\n",
    "    print('newX shape', newX.shape)\n",
    "    # definição da variável alvo \n",
    "    newY = dataframe['imdb_score']\n",
    "    # aplicação da transformação polinomial nos dados de entrada X e imprimindo o tamanho\n",
    "    df_temp = pd.DataFrame(poly.fit_transform(newX[numeric_columns]))\n",
    "    print('\\ndf_temp shape', df_temp.shape)\n",
    "    # concatenando os dados numericos polinomiais com as variávis dummies\n",
    "    df_temp = pd.concat([df_temp,dataframe[dummies].reset_index()], axis=1)\n",
    "    print('df_temp shape', df_temp.shape)\n",
    "    # splitando os dados em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_temp,newY,test_size=0.2)\n",
    "    # imprimindo o tamanho dos dados de entrada X de treino e teste\n",
    "    print('\\ntrain shape', X_train.shape)\n",
    "    print('teste shape', X_test.shape)\n",
    "    # ajustando o modelo\n",
    "    modelo.fit(X_train, y_train)\n",
    "    # fazendo predições para os dados de treino\n",
    "    yhat = modelo.predict(X_train)\n",
    "    # fazendo predições para os dados de teste\n",
    "    yhat_test = modelo.predict(X_test[list(X_train)])\n",
    "    # imprimindo as métricas e retornando o modelo\n",
    "    print_metrics(y_train, yhat, y_test, yhat_test)\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar os modelos de Regressão Linear e Random Forrest com diferentes graus de funções polinomiais para saber qual é a melhor combinação para ser adotada em nossas predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodando a função para modelagem com o modelo linear\n",
    "# parametriando a função polinomial com grau 1\n",
    "model1 = run_model_with_poly(df, 1, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodando a função para modelagem com o modelo linear\n",
    "# parametriando a função polinomial com grau 2\n",
    "model2 = run_model_with_poly(df, 2, LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodando a função para modelagem com o modelo Random Forrest\n",
    "# parametriando a função polinomial com grau 1\n",
    "model3 = run_model_with_poly(df, 1, RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodando a função para modelagem com o modelo random forrest\n",
    "# parametriando a função polinomial com grau 2\n",
    "model4 = run_model_with_poly(df, 2, RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rodando a função para modelagem com o modelo random forrest\n",
    "# parametriando a função polinomial com grau 2\n",
    "model5 = run_model_with_poly(df, 3, RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando nossos dois modelos (MQO vs Arvores Aleatórias) parece que as segunda consegue superar as métricas no treino e no teste então vamos usar esse modelo em produção (ou seja, para realizar nosso trabalho).\n",
    "\n",
    "Estamos buscando uma convicção de que o modelo treinado tem uma boa relação entre o viés e variância com a parametrização pela complexidade do próprio modelo. \n",
    "\n",
    "Novamente, quanto mais complexo mais se consegue capturar efeitos não previsto pelo modelo, como comportamentos não lineares e interação das variáveis.\n",
    "\n",
    "Porém, tornando o modelo menos genérico, ou seja, diminuindo os scores dos dados de teste, devemos treinar o modelo final com os parâmetros que conseguiram os melhores resultados nos dados de teste.\n",
    "\n",
    "## Escolhendo o melhor modelo\n",
    "\n",
    "Dos nossos testes parece que a melhor combinação foi um RandomForestRegressor com polinomios = 1. Então vamos fazer o treinamento do modelo final com TODOS os dados (agora não podemos mais confiar nas métrica pois não teremos dados de teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo os dados\n",
    "Y = df['imdb_score']\n",
    "X = df.drop(['imdb_score'], axis=1)\n",
    "# rodando o melhor modelo\n",
    "final_model = RandomForestRegressor().fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passo 5 - Fazer Predições\n",
    "\n",
    "Depois que temos o melhor modelo treinado vamos pegar os dados do mundo real e fazer predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados originais, criando uma cópia e verificando o DataFrame\n",
    "data_raw = pd.read_csv('../../99 Datasets/imdb_test.zip')\n",
    "data = data_raw.copy(deep=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse nosso caso, temos o target nos dados \"reais\", geralmente não o teriamos. Aqui poderemos saber a perfomance do nosso algoritmo.\n",
    "\n",
    "## Fazendo os mesmos tratamento nos dados \n",
    "\n",
    "Todos os tratamentos de dados que fizemos no dataset original precisamos refazer aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropando as colunas que causam overfit\n",
    "data = data.drop(columns_to_drop, axis=1)\n",
    "# preenchendo as colunas numéricas com a média\n",
    "data[list(df_numeric)] = data[list(df_numeric)].fillna(data.mean())\n",
    "# preenchendo os campos nulos\n",
    "data = data.fillna('na')\n",
    "# criando DUMMIES para as colunas categóricas\n",
    "data = pd.get_dummies(data)\n",
    "# separando os dados da variável alvo no dataset Y\n",
    "Ydata = data['imdb_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando o número de colunas de X\n",
    "len(list(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando o numero de colunas do DataFrame\n",
    "len(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para zerar as DUMMIES que não estão em X\n",
    "for i, dummy in enumerate(list(X)):\n",
    "    if dummy not in list(data):\n",
    "        data[dummy] = 0\n",
    "\n",
    "print(f'dummies adicionadas:{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atribuindo todas as DUMMIES aos dados de entrada X\n",
    "Xdata = data[list(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo as predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazendo as predições com o melhor modelo já treinado\n",
    "yhat = final_model.predict(Xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfeito! Temos um array com os valores previstos\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como podemos saber de qual filme se trata cada um dos score previstos?\n",
    "\n",
    "\n",
    "- o algoritmo não diz qual é o nome do filme, mas a ordem em que passamos os dados se mantém ...\n",
    "\n",
    "\n",
    "- então podemos adicionar uma coluna no nosso dataframe original com as predições!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Respondendo a pergunta do nosso Desafio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adicionando os valores previstos ao DataFrame original\n",
    "data_raw['score_previsto'] = yhat\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando um novo DataFrame com o Score Previsto, título do filme e o orçamento\n",
    "result = data_raw[['movie_title','budget','score_previsto']]\n",
    "# criando uma nova coluna RATIO com o orçamento (em USD Mi) dividido pelo Score Previsto\n",
    "result['ratio'] = (result['budget']/1000000)/(result['score_previsto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a solução, devemos pensar que os filmens com menos Bugdet irão custar mais barato, e os filmes com maior Score Previsto são aqueles que tem melhor apreciação do público, portanto o RATIO tem a idéia de capturar as oportunidades mais baratas e com melhor avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"piores\" 10 filmes nas regras desse desafio, são os filmes que os scores custam mais caro\n",
    "result.sort_values(by=['ratio'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"melhores\" 10 filmes nas regras desse desafio são aqueles com o menos ratio\n",
    "result.sort_values(by=['ratio'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próximo passo, colocar o modelo em produção ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
